/* agent.js â€“ two-stage RAG with MMR + GPT-4o-mini polish
   ------------------------------------------------------ */

import "dotenv/config";
import { z } from "zod";
import { ChatOpenAI } from "@langchain/openai";
import { tool } from "@langchain/core/tools";
import { MemorySaver } from "@langchain/langgraph";
import { vectorstore } from "./embeddings.js";  
import { addYTVtoVectorStore } from "./embeddings.js";
import data  from "./data.js";

/* â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” */
const dbg = await vectorstore.similaritySearch(
  "How to communicate with AI in code?",
  3               // K
);
console.log(dbg.map(d => d.metadata));
await addYTVtoVectorStore({ transcript:data[0].transcript , video_id:data[0].video_id  });
await addYTVtoVectorStore({ transcript:data[1].transcript , video_id:data[1].video_id });
/* 1. LLM instances
   ----------------------------------------------------------------------- */
const draftLLM = new ChatOpenAI({
  model: "gpt-3.5-turbo",
  temperature: 0,
  maxTokens: 256,
  timeout: 20_000,
});

const finalLLM = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0.2,
  topP: 0.95,
  presencePenalty: 0.3,
  maxTokens: 1024,
  timeout: 30_000,
});

/* 2. System prompt used in the *final* step */
const SYSTEM_PROMPT = `
You are an expert YouTube-video assistant.
â€¢ Base every answer ONLY on the supplied transcript snippets.
â€¢ Quote exact timestamps so users can re-watch the relevant moments.
â€¢ If the answer is not in the snippets, respond: â€œThe transcript doesnâ€™t cover that.â€
`;

/* 3. Retriever tool â€“ MMR, video-scoped, fewer chunks */
const RETRIEVER_K = 12;      // total chunks for polishing step
const OUTLINE_K   = 6;      // chunks for outline step

export const retrieverTool = tool(
  async ({ query }, { configurable: { video_id, k = RETRIEVER_K } }) => 
    {
    const docs = await vectorstore.maxMarginalRelevanceSearch(
      query,
      {
        
          k:k,
        fetchK: k * 4,
        filter:  { video_id: video_id} 
      }

      );
    //  const docs = await fetchChunks(query, video_id, k);
      console.log(
      "ðŸ” top chunks:",
      docs.map(d => d.pageContent.slice(0, 80))  // first 80 chars for brevity
    );
      return docs;
  },
  {
    name: "retriever",
    description:
      "Returns the most relevant transcript chunks of a YouTube video.",
    schema: z.object({ query: z.string() }),
  }
);

/* 4. Public helper â€“ ONE call from  routes/UI
   ------------------------------------------------- */
export async function answerVideoQuery({ question, video_id }) {
  /* ---- Stage 1: outline with the fast model ---- */
  const draftDocs   = await retrieverTool.invoke(
    { query: question },
    { configurable: { video_id, k: OUTLINE_K } }
  );

  const draftCtx    = draftDocs
    .map((d) => d.pageContent)
    .join("\n---\n");

  const outlineMsg  = await draftLLM.invoke([
    {
      role: "system",
      content:
        "Make a concise, bullet-point outline that will help another model " +
        "answer the userâ€™s question. Use the snippets only.",
    },
    {
      role: "user",
      content:
        `Question: ${question}\n\nTranscript snippets:\n${draftCtx}`,
    },
  ]);

  /* ---- Stage 2: polish with GPT-4o-mini ---- */
  const fullDocs    = await retrieverTool.invoke(
    { query: question },
    { configurable: { video_id } }          // k defaults to RETRIEVER_K
  );

  const fullCtx     = fullDocs
    .map((d) => d.pageContent)
    .join("\n---\n");

  const finalMsg    = await finalLLM.invoke([
    { role: "system", content: SYSTEM_PROMPT },
    {
      role: "assistant",
      content:
        `Outline (generated by a helper model):\n${outlineMsg.content}`,
    },
    {
      role: "user",
      content:
        `Answer the question thoroughly.\n\n` +
        `QUESTION: ${question}\n\n` +
        `TRANSCRIPT SNIPPETS:\n${fullCtx}`,
    },
  ]);

  return finalMsg.content;     // <-- ready for your UI / API response
}

/* 5. (Optionally) keep the MemorySaver if you log internal steps */
export const checkpointer = new MemorySaver();
